{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"webscrape.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"TqjbkktYs_oD","colab_type":"code","colab":{}},"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup, NavigableString, Tag\n","\n","site = \"https://www.fool.com\"\n","list_url_prefix = \"https://www.fool.com/earnings-call-transcripts/?page=\"\n","\n","import re\n","import pickle\n","import os\n","import pandas as pd\n","\n","#%% returns tags between two tags\n","def between(cur, end):\n","    while cur and cur != end:\n","        if (isinstance(cur, Tag)) and (set([z.name for z in cur.contents])=={None}) and (cur.name != \"script\"):\n","            yield cur\n","        \n","        cur = cur.next_element \n","\n","#%% build list of call transcript links\n","links = []\n","\n","for i in range(5):\n","   page = requests.get(list_url_prefix+str(i))\n","   soup = BeautifulSoup(page.text, \"html.parser\")\n","   items = soup.find_all(class_=\"card-image\")\n","   \n","   for item in items:\n","       links.append(site+item.a[\"href\"])\n","\n","#%%\n","with open('C:/data/calllinks.txt', 'w') as f:\n","   for item in links:\n","       f.write(\"%s\\n\" % item)\n","\n","#%% read csv\n","f = open('C:/data/calllinks.txt', 'r')\n","links = f.read().splitlines()\n","f.close()\n","\n","#%%\n","#import random\n","#linkstest = random.sample(links,3000)\n","\n","#%% get the pickle of previously processed calls\n","# used to pick up where we left off\n","if os.path.exists(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\"):\n","    pfile = open(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\", \"rb\") \n","    calls = pickle.load(pfile)                      \n","    pfile.close()\n","\n","#%%\n","#calls = {}\n","\n","#%% processes calls\n","qa_pattern = \"Question.*Answer\"\n","duration_pattern = \"Duration:.*[M|m]inutes\"\n","participants_pattern = \"Call [P|p]articipants\"\n","prepared_remarks_pattern = \"Prepared Remarks:\"\n","\n","for link in links: #linkstest:#links[0:1000]:\n","    if link not in calls.keys():\n","        page = requests.get(link)\n","        soup = BeautifulSoup(page.text, \"html.parser\")\n","        header1 = soup.find(class_=\"usmf-new article-header\").find(\"h1\").text\n","        header2 = soup.find(class_=\"usmf-new article-header\").find(\"h2\").text\n","        body = soup.find(class_=\"article-content\")\n","        print(header1)\n","        print(header2)\n","        \n","        #############################################################################################################################################\n","        # metadata\n","        #############################################################################################################################################\n","        metadata = {}\n","        \n","        # storing important pieces of text to allow for any parsing after pickling\n","        metadata[\"header1\"] = header1\n","        metadata[\"header2\"] = header2\n","        if body.find(\"h2\", string=re.compile(\"Contents\")) != None:\n","            metadata[\"first paragraph\"] = str(body.find(\"h2\", string=re.compile(\"Contents\")).find_previous_sibling(\"p\"))\n","        else:\n","            metadata[\"first paragraph\"] = None\n","        \n","        # company\n","        metadata[\"company\"] = body.find_all(\"strong\")[0].text  \n","        \n","        # ticker and exchange\n","        metadata[\"ticker\"] = header1.split(\")\")[0].split(\"(\")[-1].strip()\n","        metadata[\"exchange\"] = None\n","        if body.find(class_=\"ticker\") != None:\n","            metadata[\"ticker\"] = body.find(class_=\"ticker\").text\n","            metadata[\"ticker\"] = metadata[\"ticker\"].replace(\"(\",\"\").replace(\")\",\"\")\n","            metadata[\"exchange\"], metadata[\"ticker\"]=metadata[\"ticker\"].split(\":\")\n","        else:\n","            ticker=body.find(string=re.compile(\"[OTC|NYSE|NASDAQ|TSX|NYSEMKT|NASDAQOTH|AMEX]: ?[A-Z]{1,5}\"))\n","            if ticker != None:\n","                metadata[\"ticker\"] = ticker.split(\":\")[1].replace(\"(\",\"\").replace(\")\",\"\").strip()\n","                metadata[\"exchange\"] = ticker.split(\":\")[0].replace(\"(\",\"\").replace(\")\",\"\").strip()\n","        \n","        # earnings period\n","        metadata[\"period\"] = None\n","        match = re.search(\"(Q[1-4]|FY) ?20[0-9][0-9]\", header1)\n","        if match:\n","            metadata[\"period\"] = match.group(0).strip()\n","        \n","        # earnings period end date\n","        metadata[\"period_end_date\"] = header2.split(\"ending\")[-1].replace(\".\",\"\").strip()\n","        \n","        # call date\n","        metadata[\"call date\"] = None\n","        if body.find(id=\"date\") != None:\n","            metadata[\"call date\"] = body.find(id=\"date\").text\n","        elif metadata[\"first paragraph\"] != None:\n","            match = re.search(\"(\\\"date\\\"|/)> ?.*[0-9]?[0-9],? ?20[0-9][0-9]\", metadata[\"first paragraph\"])\n","            if match:\n","                metadata[\"call date\"] = match.group(0).split(\">\")[-1].strip()\n","\n","        # call time\n","        metadata[\"call time\"] = None\n","        if body.find(id=\"time\") != None:\n","            metadata[\"call time\"] = body.find(id=\"time\").text\n","        elif metadata[\"first paragraph\"] != None:\n","            match = re.search(\" ?[0-9]?[0-9]:[0-9][0-9] ?(a\\.?m\\.?|p\\.?m\\.?)( ?ET)?\", metadata[\"first paragraph\"])\n","            if match:\n","                metadata[\"call time\"] = match.group(0).strip()           \n","        \n","        # call participants (find h2 with text.contains(\"Call particpants\"))\n","        if body.find(\"h2\", string=re.compile(\"Call [P|p]articipants\")) != None:\n","            call_participant_start = body.find(\"h2\", string=re.compile(\"Call [P|p]articipants\")).find_next_siblings()\n","            metadata[\"call participants\"] = []\n","            for res in call_participant_start:           \n","                if (res.find(\"strong\") != None) & (res.find(\"em\") != None) & (res.text != \"More \"+metadata[\"ticker\"]+\" analysis\"):\n","                    participant_name = res.find(\"strong\").text\n","                    company_and_title = res.find(\"em\").text.split(\"--\")\n","                    if len(company_and_title)==2:\n","                        participant_company = res.find(\"em\").text.split(\"--\")[0].strip()\n","                        participant_title = res.find(\"em\").text.split(\"--\")[1].strip()\n","                    else:\n","                        participant_company = metadata[\"company\"]\n","                        participant_title = res.find(\"em\").text.split(\"--\")[0].strip()\n","                        \n","                    metadata[\"call participants\"].append((participant_name, participant_company, participant_title))\n","                elif (res.text == \"More \"+metadata[\"ticker\"]+\" analysis\"):\n","                    break\n","            metadata[\"call participants\"]=pd.DataFrame(metadata[\"call participants\"], columns=[\"Speaker\",\"Company\",\"Title\"])\n","        else:\n","            metadata[\"call participants\"] = None       \n","        \n","        # call duration\n","        metadata[\"duration\"] = None\n","        for paragraph in body.find_all(\"p\"):\n","            match = re.search(duration_pattern, paragraph.text)\n","            if match and (len(match.group(0).split())==3):\n","                metadata[\"duration\"] = match.group(0).split()[-2]\n","    \n","    \n","        #############################################################################################################################################\n","        # text\n","        #############################################################################################################################################\n","        # prepared remarks\n","        prepared_remarks = []\n","        if body.find('h2', string=re.compile(prepared_remarks_pattern)) != None:\n","            prepared_remarks = [(tag.name,tag.text) for tag in \n","                                        between(body.find('h2', string=re.compile(prepared_remarks_pattern)).next_element, \n","                                                body.find('h2', string=re.compile(qa_pattern)))]\n","        elif body.find('strong', string=re.compile(prepared_remarks_pattern)) != None:\n","            prepared_remarks = [(tag.name,tag.text) for tag in \n","                                        between(body.find('strong', string=re.compile(prepared_remarks_pattern)).next_element, \n","                                                body.find('strong', string=re.compile(qa_pattern)))]\n","        text_prepared_remarks = []\n","        for item in prepared_remarks:\n","            tag_type = item[0]\n","            text = item[1]\n","            if tag_type == \"strong\":\n","                speaker=text\n","            elif tag_type == \"p\":\n","                text_prepared_remarks.append((speaker,\"Prepared Remarks\",text))\n","            \n","        # call questions and answers\n","        text_qa = []\n","        if body.find('h2', string=re.compile(qa_pattern)) != None:\n","            qa = [(tag.name,tag.text) for tag in \n","                                        between(body.find('h2', string=re.compile(qa_pattern)), \n","                                                body.find('strong', string=re.compile(\"Duration\")))]\n","            for item in qa:\n","                tag_type = item[0]\n","                text = item[1]\n","                if tag_type == \"strong\":\n","                    speaker=text\n","                elif tag_type == \"p\":\n","                    text_qa.append((speaker,\"Q&A\",text))\n","    \n","        text_all = pd.DataFrame(text_prepared_remarks + text_qa, columns=[\"Speaker\",\"Call Section\",\"Text\"])\n","        \n","        calls[link]={\"metadata\":metadata, \"text\":text_all}\n","        print()\n","\n","if os.path.exists(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\"):\n","    os.remove(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\")\n","pfile = open(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\", \"ab\")\n","pickle.dump(calls, pfile)                  \n","pfile.close()\n","\n","#%% in case you stop in between a run\n","if os.path.exists(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\"):\n","    os.remove(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\")\n","pfile = open(\"C:/Users/MLabadie/Downloads/NLP Project Research/ProcessedCallsPickle\", \"ab\")\n","pickle.dump(calls, pfile)                  \n","pfile.close()\n","\n","\n","\n","\n","\n","\n","\n","#%% testing\n","link=\"https://www.fool.com/earnings/call-transcripts/2019/02/25/carters-inc-cri-q4-2018-earnings-conference-call-t.aspx\"\n","page = requests.get(link)\n","soup = BeautifulSoup(page.text, \"html.parser\")\n","header1 = soup.find(class_=\"usmf-new article-header\").find(\"h1\").text\n","header2 = soup.find(class_=\"usmf-new article-header\").find(\"h2\").text\n","body = soup.find(class_=\"article-content\")\n","\n","#%%\n","body.find(string=re.compile(\"[OTC|NYSE|NASDAQ|TSX|NYSEMKT|NASDAQOTH|AMEX]:.{1,5}\"))"],"execution_count":0,"outputs":[]}]}