{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8. Keywords.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9A3Ujv2QqOCn",
        "L35fZdIEDa39",
        "5O0UPGv5YqXn",
        "VRpO5FrJC1D7",
        "5k8oRrtyCAZd",
        "6A3ukz8aX2U9",
        "pIbaHaWB3tO2",
        "AhJ2lc3G3vTv",
        "gKDKfyCmatd0",
        "U2BFG-CV2RNV",
        "HAn8KUSQPhPA",
        "athwtQ8KZqF3",
        "4S-DYaOMTI1S"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikelabadie/Earnings_Call_Transcripts/blob/master/8_Keywords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9A3Ujv2QqOCn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bring In Data and Libraries\n",
        "\n",
        "*   Segments calls by quarter\n",
        "*   Builds a vocabulary of every word seen on any call\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "L35fZdIEDa39",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Bring in Data"
      ]
    },
    {
      "metadata": {
        "id": "-tH_oi6IrQz-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pprint\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhM5JD-kqJw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_directory = \"/content/gdrive/My Drive/DATS6450 - Labadie - Data/\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if os.path.exists(data_directory+\"Pickles/metadata.pkl\"):\n",
        "    pfile = open(data_directory+\"Pickles/metadata.pkl\", \"rb\")\n",
        "    df_metadata = pickle.load(pfile)                 \n",
        "    pfile.close()\n",
        "\n",
        "if os.path.exists(data_directory+\"Pickles/text_paragraph.pkl\"):\n",
        "    pfile = open(data_directory+\"Pickles/text_paragraph.pkl\", \"rb\")\n",
        "    df_text_prepremarks = pickle.load(pfile)                 \n",
        "    pfile.close()\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5O0UPGv5YqXn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Segment Calls by Quarter"
      ]
    },
    {
      "metadata": {
        "id": "oGfXRb_hG8Kc",
        "colab_type": "code",
        "outputId": "326c8606-a0b7-4b85-b202-24e0b9410cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from dateutil.parser import parse\n",
        "\n",
        "now = datetime.now()\n",
        "today = datetime(now.year, now.month, now.day)\n",
        "apr_2019 = datetime(2019,4,1)\n",
        "jan_2019 = datetime(2019,1,1)\n",
        "oct_2018 = datetime(2018,10,1)\n",
        "jul_2018 = datetime(2018,7,1)\n",
        "apr_2018 = datetime(2018,4,1)\n",
        "jan_2018 = datetime(2018,1,1)\n",
        "oct_2017 = datetime(2017,10,1)\n",
        "\n",
        "q4_2017_calls = df_metadata[df_metadata.apply(lambda row: (jan_2018 > row[\"call date\"] >= oct_2017), axis=1)]\n",
        "q1_2018_calls = df_metadata[df_metadata.apply(lambda row: (apr_2018 > row[\"call date\"] >= jan_2018), axis=1)]\n",
        "q2_2018_calls = df_metadata[df_metadata.apply(lambda row: (jul_2018 > row[\"call date\"] >= apr_2018), axis=1)]\n",
        "q3_2018_calls = df_metadata[df_metadata.apply(lambda row: (oct_2018 > row[\"call date\"] >= jul_2018), axis=1)]\n",
        "q4_2018_calls = df_metadata[df_metadata.apply(lambda row: (jan_2019 > row[\"call date\"] >= oct_2018), axis=1)]\n",
        "q1_2019_calls = df_metadata[df_metadata.apply(lambda row: (row[\"call date\"] >= jan_2019), axis=1)]\n",
        "\n",
        "print(\"Q4 2017 Calls:\",q4_2017_calls.shape)\n",
        "print(\"Q1 2018 Calls:\",q1_2018_calls.shape)\n",
        "print(\"Q2 2018 Calls:\",q2_2018_calls.shape)\n",
        "print(\"Q3 2018 Calls:\",q3_2018_calls.shape)\n",
        "print(\"Q4 2018 Calls:\",q4_2018_calls.shape)\n",
        "print(\"Q1 2019 Calls:\",q1_2019_calls.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q4 2017 Calls: (137, 16)\n",
            "Q1 2018 Calls: (306, 16)\n",
            "Q2 2018 Calls: (355, 16)\n",
            "Q3 2018 Calls: (546, 16)\n",
            "Q4 2018 Calls: (1404, 16)\n",
            "Q1 2019 Calls: (2328, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VRpO5FrJC1D7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Get list of all unique words used across all calls"
      ]
    },
    {
      "metadata": {
        "id": "L25ge-GrCzSb",
        "colab_type": "code",
        "outputId": "b51886bb-5229-49a8-b0ea-d7bbb3f04a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "docs = df_metadata[\"Prepared Remarks Text\"]\n",
        "def get_vocab(docs):\n",
        "    vectorizer = CountVectorizer(binary=True, lowercase=True)\n",
        "    vectors = vectorizer.fit_transform(docs)\n",
        "    return vectorizer.get_feature_names()  \n",
        "  \n",
        "full_vocab = get_vocab(docs)\n",
        "len(full_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "5k8oRrtyCAZd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Build Generic Earnings Call Stop Word Sets at Call/Paragraph Levels\n",
        "* Words that are in 75% of calls\n",
        "* Words that are in 7.5% of paragraphs"
      ]
    },
    {
      "metadata": {
        "id": "GUUfvoZ23bpX",
        "colab_type": "code",
        "outputId": "802d4179-063f-483b-90a3-5478ef97eec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "cell_type": "code",
      "source": [
        "# build high-level stop word list at the entire call level\n",
        "docs = df_metadata[\"Prepared Remarks Text\"]\n",
        "vectorizer = CountVectorizer(binary=True, lowercase=True, min_df=0.75)\n",
        "vectors = vectorizer.fit_transform(docs)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "results = pd.DataFrame(data=vectors.toarray(), index=docs.index, columns=vectorizer.get_feature_names())\n",
        "call_stopwords = results.sum(axis=0)\n",
        "call_stopwords = list(call_stopwords.index)\n",
        "print(len(call_stopwords))\n",
        "print(np.array(call_stopwords))\n",
        "print()\n",
        "\n",
        "# build high-level stop word list at the paragraph level\n",
        "docs = df_text_prepremarks[\"Text\"]\n",
        "vectorizer = CountVectorizer(binary=True, lowercase=True, min_df=0.075)\n",
        "vectors = vectorizer.fit_transform(docs)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "results = pd.DataFrame(data=vectors.toarray(), index=docs.index, columns=vectorizer.get_feature_names())\n",
        "paragraph_stopwords = results.sum(axis=0)\n",
        "paragraph_stopwords = list(paragraph_stopwords.index)\n",
        "print(len(paragraph_stopwords))\n",
        "print(np.array(paragraph_stopwords))\n",
        "\n",
        "# combine stopwords from both levels\n",
        "combined_stopwords = set(paragraph_stopwords).union(set(call_stopwords))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161\n",
            "['10' '12' '20' '2017' '2018' '2019' 'about' 'across' 'actual' 'addition'\n",
            " 'additional' 'all' 'also' 'an' 'and' 'any' 'approximately' 'are' 'as'\n",
            " 'at' 'back' 'based' 'basis' 'be' 'been' 'before' 'believe' 'both'\n",
            " 'business' 'but' 'by' 'call' 'can' 'capital' 'cash' 'company' 'compared'\n",
            " 'continue' 'continued' 'cost' 'current' 'driven' 'due' 'during'\n",
            " 'earnings' 'end' 'everyone' 'expect' 'expected' 'financial' 'first' 'for'\n",
            " 'forward' 'fourth' 'from' 'full' 'future' 'gaap' 'good' 'growth' 'had'\n",
            " 'has' 'have' 'high' 'higher' 'impact' 'in' 'including' 'income'\n",
            " 'increase' 'increased' 'into' 'is' 'it' 'joining' 'last' 'like' 'line'\n",
            " 'll' 'long' 'looking' 'lower' 'made' 'margin' 'market' 'may' 'me'\n",
            " 'million' 'more' 'most' 'net' 'new' 'non' 'not' 'now' 'of' 'on' 'one'\n",
            " 'operating' 'or' 'other' 'our' 'over' 'per' 'performance' 'primarily'\n",
            " 'prior' 'provide' 'quarter' 'questions' 'range' 'rate' 're' 'related'\n",
            " 'release' 'result' 'results' 'revenue' 'sales' 'second' 'see' 'share'\n",
            " 'significant' 'some' 'statements' 'strong' 'term' 'than' 'thank' 'that'\n",
            " 'the' 'their' 'these' 'third' 'this' 'those' 'through' 'time' 'to'\n",
            " 'today' 'total' 'turn' 'two' 'up' 'us' 'value' 've' 'very' 'was' 'we'\n",
            " 'website' 'well' 'were' 'which' 'while' 'will' 'with' 'would' 'year'\n",
            " 'years' 'you']\n",
            "\n",
            "109\n",
            "['10' '2017' '2018' '2019' 'about' 'adjusted' 'all' 'also' 'an' 'and'\n",
            " 'approximately' 'are' 'as' 'at' 'basis' 'be' 'both' 'business' 'but' 'by'\n",
            " 'call' 'cash' 'company' 'compared' 'continue' 'customers' 'due' 'during'\n",
            " 'earnings' 'end' 'expect' 'financial' 'first' 'for' 'forward' 'fourth'\n",
            " 'from' 'full' 'good' 'growth' 'has' 'have' 'higher' 'impact' 'in'\n",
            " 'including' 'increase' 'increased' 'into' 'is' 'it' 'last' 'like' 'll'\n",
            " 'margin' 'market' 'million' 'more' 'net' 'new' 'not' 'now' 'of' 'on'\n",
            " 'one' 'operating' 'or' 'other' 'our' 'over' 'per' 'performance' 'quarter'\n",
            " 'rate' 're' 'results' 'revenue' 'sales' 'second' 'share' 'so' 'some'\n",
            " 'strong' 'than' 'that' 'the' 'these' 'third' 'this' 'through' 'time' 'to'\n",
            " 'today' 'total' 'turn' 'up' 'us' 've' 'very' 'was' 'we' 'well' 'were'\n",
            " 'which' 'while' 'will' 'with' 'year' 'you']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6A3ukz8aX2U9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Word Frequency Functions\n",
        "*   This is where the magic happens.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pIbaHaWB3tO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Helpers\n",
        "Many of these are not currently used, but were at one point useful during development."
      ]
    },
    {
      "metadata": {
        "id": "PEnfD4CSTAEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_calls(from_date, to_date, category=None, category_filter=None):\n",
        "    calls = df_metadata[df_metadata.apply(lambda row: (to_date > row[\"call date\"] >= from_date), axis=1)]\n",
        "    if category is not None:\n",
        "        calls = calls[calls[category]==category_filter]\n",
        "    return calls\n",
        "\n",
        "def get_calls_for_shared_tickers(calls1, calls2):\n",
        "    tickers = list(set(calls2[\"ticker\"]).intersection(set(calls1[\"ticker\"])))\n",
        "    calls1 = calls1[calls1[\"ticker\"].isin(tickers)]\n",
        "    calls2 = calls2[calls2[\"ticker\"].isin(tickers)]\n",
        "    return calls1, calls2\n",
        "\n",
        "def get_calls_interest_reference(interest_date_from, interest_date_to, reference_date_from, referece_date_to, cat_type, cat_filter, shared=False):\n",
        "    interest_calls = get_calls(interest_date_from, interest_date_to, cat_type, cat_filter)\n",
        "    reference_calls = get_calls(reference_date_from, referece_date_to, cat_type, cat_filter)\n",
        "    if shared:\n",
        "        reference_calls, interest_calls = get_calls_for_shared_tickers(reference_calls, interest_calls)\n",
        "    return interest_calls, reference_calls \n",
        "  \n",
        "def get_norm_count_vector(docs, index, vocab, stopwords):\n",
        "    vectorizer = CountVectorizer(binary=False, lowercase=True, vocabulary=vocab, stop_words=stopwords)\n",
        "    vectors = vectorizer.fit_transform(docs)\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    results = pd.DataFrame(data=vectors.toarray(), index=index, columns=vectorizer.get_feature_names())\n",
        "    results=results.div(results.sum(axis=1), axis=0)\n",
        "    results.fillna(0,inplace=True)\n",
        "    return results\n",
        "  \n",
        "def get_tfidf_vector(docs, index, vocab, stopwords):\n",
        "    vectorizer = TfidfVectorizer(binary=False, lowercase=True, vocabulary=vocab, stop_words=stopwords)\n",
        "    vectors = vectorizer.fit_transform(docs)\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    results = pd.DataFrame(data=vectors.toarray(), index=index, columns=vectorizer.get_feature_names())\n",
        "    results.fillna(0,inplace=True)\n",
        "    return results\n",
        "  \n",
        "# get top 20 changes up and down\n",
        "def get_keywords_norm_count(first_qtr_calls, second_qtr_calls):\n",
        "    # build relative frequency word vector for first period\n",
        "    docs = first_qtr_calls[\"Prepared Remarks Text\"]\n",
        "    first_qtr_results = get_norm_count_vector(docs,first_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "\n",
        "    # build relative frequency word vector for second period\n",
        "    docs = second_qtr_calls[\"Prepared Remarks Text\"]\n",
        "    second_qtr_results = get_norm_count_vector(docs,second_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "\n",
        "    # get the difference in relative frequency by company/word\n",
        "    diff = (second_qtr_results.mean() - first_qtr_results.mean()).sort_values()\n",
        "    return diff.head(20).append(diff.tail(20))\n",
        "  \n",
        "# get top 20 changes up and down\n",
        "def get_keywords_tfidf(first_qtr_calls, second_qtr_calls):\n",
        "    # build relative frequency word vector for first quarter\n",
        "    docs = first_qtr_calls[\"Prepared Remarks Text\"]\n",
        "    first_qtr_results = get_tfidf_vector(docs, first_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "\n",
        "    # build relative frequency word vector for second quarter\n",
        "    docs = second_qtr_calls[\"Prepared Remarks Text\"]\n",
        "    second_qtr_results = get_tfidf_vector(docs, second_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "\n",
        "    # get the difference in relative frequency by company/word\n",
        "    diff = (second_qtr_results.mean() - first_qtr_results.mean()).sort_values()\n",
        "    return diff.head(20), diff.tail(20).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "def get_count_vector(docs, index, vocab, stopwords):\n",
        "    vectorizer = CountVectorizer(binary=False, lowercase=True, vocabulary=vocab, stop_words=stopwords)\n",
        "    vectors = vectorizer.fit_transform(docs)\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    results = pd.DataFrame(data=vectors.toarray(), index=index, columns=vectorizer.get_feature_names())\n",
        "    results.fillna(0,inplace=True)\n",
        "    return results\n",
        "\n",
        "# get top 20 changes up and down\n",
        "def get_keywords_count(first_qtr_calls, second_qtr_calls, additional_stopwords=[]): \n",
        "    # build relative frequency word vector for first period\n",
        "    docs = first_qtr_calls[\"Prepared Remarks Text\"]\n",
        "    first_qtr_results = get_count_vector(docs,first_qtr_calls[\"ticker\"], \n",
        "                                         full_vocab, list(combined_stopwords)+additional_stopwords)\n",
        "\n",
        "    # build relative frequency word vector for second period\n",
        "    docs = second_qtr_calls[\"Prepared Remarks Text\"]\n",
        "    second_qtr_results = get_count_vector(docs,second_qtr_calls[\"ticker\"], \n",
        "                                          full_vocab, list(combined_stopwords)+additional_stopwords)\n",
        "\n",
        "    # get the difference in relative frequency by company/word\n",
        "    diff = (second_qtr_results.sum() - first_qtr_results.sum()).sort_values()\n",
        "    return diff.head(20), diff.tail(20).sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AhJ2lc3G3vTv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Primary Functions"
      ]
    },
    {
      "metadata": {
        "id": "g6SAldyHrUyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_stopwords_for_all_calls_during_time_period(interest_date_from, interest_date_to,relative_freq_to_max=0.5):\n",
        "    calls = get_calls(interest_date_from, interest_date_to)\n",
        "    docs=calls[\"Prepared Remarks Text\"]\n",
        "    all_results = get_count_vector(docs,calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "    all_results = all_results.sum().sort_values(ascending=False)\n",
        "    num=all_results[0]*relative_freq_to_max\n",
        "    all_results = list(all_results[all_results>num].index)\n",
        "    return all_results\n",
        "    \n",
        "def get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                               cat_type, cat_filter, \n",
        "                               filter_reference=True, shared=False, additional_stopwords=None, \n",
        "                               method=\"counts\", number_words_to_return=None):\n",
        "  \n",
        "    # build the sets of calls to compare\n",
        "    interest_calls = get_calls(interest_date_from, interest_date_to, cat_type, cat_filter)\n",
        "    \n",
        "    \n",
        "    # should the reference period use the same filter\n",
        "    if filter_reference:\n",
        "      reference_calls = get_calls(reference_date_from, referece_date_to, cat_type, cat_filter)\n",
        "    else:\n",
        "      reference_calls = get_calls(reference_date_from, referece_date_to, None, None)\n",
        "    \n",
        "    # remove interest calls from the reference calls\n",
        "    reference_calls = reference_calls[~(reference_calls[\"Link\"].isin(interest_calls[\"Link\"]))]\n",
        "    \n",
        "    # filter to calls with tickers in both reference and interest\n",
        "    if shared:\n",
        "        reference_calls, interest_calls = get_calls_for_shared_tickers(reference_calls, interest_calls)\n",
        "    \n",
        "    \n",
        "    # determine what method will be used to identify changes\n",
        "    if method==\"counts\":\n",
        "        docs=reference_calls[\"Prepared Remarks Text\"]\n",
        "        reference_results = get_count_vector(docs,reference_calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "        reference_results = reference_results.sum().sort_values(ascending=False)\n",
        "\n",
        "        docs=interest_calls[\"Prepared Remarks Text\"]\n",
        "        interest_results = get_count_vector(docs, interest_calls[\"ticker\"], full_vocab, combined_stopwords)\n",
        "        interest_results = interest_results.sum().sort_values(ascending=False)\n",
        "        \n",
        "        # convert counts to percent of total and get difference\n",
        "        interest_results_pct=interest_results.div(interest_results.sum())\n",
        "        reference_results_pct=reference_results.div(reference_results.sum())\n",
        "        diff = interest_results_pct - reference_results_pct        \n",
        "    elif method == \"tfidf\":\n",
        "        reference_doc=\" \".join(reference_calls[\"Prepared Remarks Text\"])\n",
        "        interest_doc=\" \".join(interest_calls[\"Prepared Remarks Text\"])\n",
        "        docs = [reference_doc,interest_doc]\n",
        "\n",
        "        vectorizer = TfidfVectorizer(binary=False, lowercase=True, stop_words=combined_stopwords)\n",
        "        vectors = vectorizer.fit_transform(docs)\n",
        "        feature_names = vectorizer.get_feature_names()\n",
        "        results = pd.DataFrame(data=vectors.toarray(), columns=vectorizer.get_feature_names())\n",
        "        results.fillna(0,inplace=True)\n",
        "        diff = results.loc[1]-results.loc[0]\n",
        "    \n",
        "    \n",
        "    # drop words from results\n",
        "    if additional_stopwords is not None:\n",
        "        diff=diff.drop(additional_stopwords, errors=\"ignore\")\n",
        "    \n",
        "    diff = diff.sort_values(ascending=False)\n",
        "    if number_words_to_return is not None:  \n",
        "        diff = diff.head(number_words_to_return)\n",
        "    \n",
        "    return diff, interest_calls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPxKIICj4dRd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Compare Q1 to Q4 for All Calls"
      ]
    },
    {
      "metadata": {
        "id": "hZltqUIv4sBd",
        "colab_type": "code",
        "outputId": "212bcecf-1bfc-47a3-b9ef-30b19390d0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "cell_type": "code",
      "source": [
        "interest_date_from, interest_date_to = jan_2019, today\n",
        "reference_date_from, referece_date_to = oct_2018, jan_2019\n",
        "\n",
        "words, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                                                   None, None, filter_reference=False, shared=True, \n",
        "                                                   additional_stopwords=None, method=\"tfidf\", number_words_to_return=30)\n",
        "\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "december        0.040954\n",
              "q4              0.036710\n",
              "billion         0.029940\n",
              "january         0.020006\n",
              "half            0.018602\n",
              "q1              0.016975\n",
              "2020            0.014725\n",
              "investments     0.013667\n",
              "31              0.013646\n",
              "throughout      0.012652\n",
              "record          0.010387\n",
              "tax             0.010301\n",
              "march           0.008797\n",
              "february        0.008317\n",
              "flow            0.008055\n",
              "31st            0.008029\n",
              "dividend        0.007910\n",
              "annual          0.007830\n",
              "program         0.007180\n",
              "decline         0.006507\n",
              "shareholders    0.006431\n",
              "investment      0.006339\n",
              "grow            0.006318\n",
              "plan            0.005985\n",
              "off             0.005692\n",
              "strategy        0.005466\n",
              "ended           0.005289\n",
              "assets          0.005234\n",
              "expense         0.005219\n",
              "position        0.005168\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "gFJiW78z4nGW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Compare Q1 2019 to Q1 2018 for All Calls"
      ]
    },
    {
      "metadata": {
        "id": "jUKbVTir4se1",
        "colab_type": "code",
        "outputId": "43107a54-1213-402a-a882-43bbd279f96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "cell_type": "code",
      "source": [
        "interest_date_from, interest_date_to = jan_2019, today\n",
        "reference_date_from, referece_date_to = jan_2018, apr_2018\n",
        "\n",
        "words, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                                                   None, None, filter_reference=False, shared=True, \n",
        "                                                   additional_stopwords=None, method=\"tfidf\", number_words_to_return=30)\n",
        "\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2020         0.021440\n",
              "program      0.019953\n",
              "half         0.016679\n",
              "customer     0.016639\n",
              "industry     0.016146\n",
              "within       0.016068\n",
              "free         0.016003\n",
              "ebitda       0.014628\n",
              "product      0.012544\n",
              "demand       0.012261\n",
              "headwinds    0.012172\n",
              "flow         0.011835\n",
              "ph           0.011657\n",
              "points       0.011558\n",
              "mid          0.011459\n",
              "markets      0.011369\n",
              "growing      0.011359\n",
              "grew         0.011321\n",
              "digit        0.011168\n",
              "price        0.011101\n",
              "better       0.011084\n",
              "gross        0.010986\n",
              "down         0.010921\n",
              "19           0.010861\n",
              "billion      0.010808\n",
              "top          0.010682\n",
              "saw          0.010653\n",
              "number       0.010617\n",
              "progress     0.010601\n",
              "focus        0.010277\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "aTaVftJ36ZPq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Compare Q1 2018 to Q1 2019 for All Calls\n",
        "What were people talking about last year that is not as important today?"
      ]
    },
    {
      "metadata": {
        "id": "kSSBCUhg6gkD",
        "colab_type": "code",
        "outputId": "aa136e2a-c9e8-4c2d-fe95-339456b85c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "cell_type": "code",
      "source": [
        "interest_date_from, interest_date_to = jan_2018, apr_2018\n",
        "reference_date_from, referece_date_to = jan_2019, today\n",
        "\n",
        "words, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                                                   None, None, filter_reference=False, shared=True, \n",
        "                                                   additional_stopwords=None, method=\"tfidf\", number_words_to_return=30)\n",
        "\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tax            0.178153\n",
              "2016           0.081244\n",
              "reform         0.053703\n",
              "deferred       0.021256\n",
              "benefit        0.021103\n",
              "just           0.020860\n",
              "think          0.020291\n",
              "do             0.015603\n",
              "act            0.014758\n",
              "if             0.014520\n",
              "law            0.012966\n",
              "change         0.012826\n",
              "federal        0.012483\n",
              "positive       0.011652\n",
              "what           0.011556\n",
              "legislation    0.011144\n",
              "there          0.011123\n",
              "corporate      0.010793\n",
              "taxes          0.010387\n",
              "loans          0.010310\n",
              "charge         0.010130\n",
              "30             0.009956\n",
              "18             0.009892\n",
              "jobs           0.009418\n",
              "still          0.009277\n",
              "benefits       0.009102\n",
              "going          0.008748\n",
              "little         0.008694\n",
              "00             0.008499\n",
              "17             0.008214\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "d1jZr6wtwMx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Compare a Group to All Other Calls during Same Period\n",
        "What do companies in this industry talk about more frequently than companies in other industries?"
      ]
    },
    {
      "metadata": {
        "id": "GWBG-9fJopt8",
        "colab_type": "code",
        "outputId": "b0d3632e-44d0-43fe-b719-fbe8c06a274d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "cell_type": "code",
      "source": [
        "cat_type=\"Industry\"\n",
        "cat_filter=\"Clothing/Shoe/Accessory Stores\"\n",
        "interest_date_from, interest_date_to = jan_2018, jan_2019\n",
        "\n",
        "industry_specific_words, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, interest_date_from, interest_date_to, \n",
        "                                                                     cat_type, cat_filter, filter_reference=False, shared=False, \n",
        "                                                                     additional_stopwords=None, method=\"tfidf\", number_words_to_return=30)\n",
        "\n",
        "industry_specific_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "store          0.218441\n",
              "stores         0.187049\n",
              "comp           0.181613\n",
              "brand          0.130975\n",
              "customer       0.104196\n",
              "retail         0.101432\n",
              "week           0.101072\n",
              "inventory      0.089128\n",
              "digital        0.083127\n",
              "brands         0.076595\n",
              "comps          0.074900\n",
              "merchandise    0.068320\n",
              "single         0.065107\n",
              "apparel        0.062251\n",
              "men            0.054929\n",
              "comparable     0.054685\n",
              "positive       0.054571\n",
              "footwear       0.051531\n",
              "traffic        0.051505\n",
              "digits         0.045357\n",
              "digit          0.045254\n",
              "categories     0.041658\n",
              "sg             0.040637\n",
              "experience     0.038962\n",
              "commerce       0.037892\n",
              "assortment     0.035704\n",
              "points         0.035445\n",
              "online         0.035318\n",
              "shift          0.034195\n",
              "53rd           0.032501\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "2WZ26xZvxJI5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Compare Company's Quarter to Previous Calls\n",
        "What is this company talking about more frequently than they have in the past?"
      ]
    },
    {
      "metadata": {
        "id": "e2i6pVjqxIgg",
        "colab_type": "code",
        "outputId": "ef7e32fa-8be4-4a04-fc51-cb656a2802e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "cell_type": "code",
      "source": [
        "cat_type=\"ticker\"\n",
        "cat_filter=\"JCP\"\n",
        "interest_date_from, interest_date_to = jan_2019, today\n",
        "reference_date_from, referece_date_to = oct_2017, jan_2019\n",
        "\n",
        "period_stopwords = get_stopwords_for_all_calls_during_time_period(interest_date_from, interest_date_to, 0.05)\n",
        "\n",
        "new_topics, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                                                        cat_type, cat_filter, filter_reference=True, shared=True, \n",
        "                                                        additional_stopwords=period_stopwords, method=\"tfidf\", number_words_to_return=25)\n",
        "\n",
        "new_topics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "apparel         0.155052\n",
              "jcpenney        0.095216\n",
              "michelle        0.082313\n",
              "join            0.054875\n",
              "shrink          0.043943\n",
              "merchandise     0.043326\n",
              "thoughtfully    0.042709\n",
              "served          0.041156\n",
              "unproductive    0.041156\n",
              "protection      0.041156\n",
              "meaningfully    0.041156\n",
              "gap             0.041156\n",
              "reestablish     0.041156\n",
              "journey         0.041156\n",
              "jill            0.037847\n",
              "swiftly         0.035996\n",
              "furniture       0.033565\n",
              "immediate       0.032948\n",
              "wants           0.029900\n",
              "training        0.027438\n",
              "showing         0.027438\n",
              "monday          0.027438\n",
              "satisfying      0.027438\n",
              "defining        0.027438\n",
              "impressed       0.027438\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "gKDKfyCmatd0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "## Testing Keywords"
      ]
    },
    {
      "metadata": {
        "id": "U2BFG-CV2RNV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "### Build Topics\n"
      ]
    },
    {
      "metadata": {
        "id": "Sviknmn1GR1s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "this_period_paragraphs = df_text_prepremarks[df_text_prepremarks.apply(lambda row: (interest_date_to > row[\"call date\"] >= interest_date_from), axis=1)]\n",
        "docs=list(this_period_paragraphs[\"Text\"])\n",
        "vectorizer = CountVectorizer(binary=True, lowercase=True, vocabulary=new_topics.index)\n",
        "vectors = vectorizer.fit_transform(docs)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "this_period_vector = pd.DataFrame(data=vectors.toarray(), index=list(this_period_paragraphs[\"Text\"].index), columns=vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h05gWVET2P4l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for word in new_topics.index[0:2]:   \n",
        "    print(\"Word:\",word)\n",
        "    \n",
        "    paragraphs_with_word = interest_calls[(interest_calls[\"Prepared Remarks Text\"].str.contains(word, flags=re.IGNORECASE, regex=True))]\n",
        "    unique_calls_with_word_industry = paragraphs_with_word[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n",
        "    print(\"    Number of Calls with Word in Group This Period:\",len(unique_calls_with_word_industry))\n",
        "    print(\"        Tickers with Word in Group This Period:\",set(unique_calls_with_word_industry[\"ticker\"].drop_duplicates()))\n",
        "    \n",
        "    paragraphs_with_word_all_calls_this_period = this_period_paragraphs.loc[this_period_vector[this_period_vector[word]==1].index]\n",
        "    unique_calls_with_word_this_period = paragraphs_with_word_all_calls_this_period[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n",
        "    print(\"    Number of Calls with Word This Period (All Calls):\",len(unique_calls_with_word_this_period))\n",
        "    print(\"        Tickers with Word This Period (All Calls):\",set(unique_calls_with_word_this_period[\"ticker\"].drop_duplicates()))\n",
        "\n",
        "    print()\n",
        "    for _, row in unique_calls_with_word_this_period.iterrows():\n",
        "        print(row[\"company\"])\n",
        "        print(row[\"Link\"])\n",
        "        pprint.pprint(list(paragraphs_with_word_all_calls_this_period[paragraphs_with_word_all_calls_this_period[\"Link\"]==row[\"Link\"]][\"Text\"]))\n",
        "        print()\n",
        "        \n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HAn8KUSQPhPA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "### Sector and Industry Lists"
      ]
    },
    {
      "metadata": {
        "id": "eKRmkJCIO0-T",
        "colab_type": "code",
        "outputId": "b88490d6-ae0e-4c2d-c3fc-c8e02e9deaaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "cell_type": "code",
      "source": [
        "print(sectors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Finance' 'Technology' 'Health Care' 'Consumer Services'\n",
            " 'Public Utilities' 'Consumer Non-Durables' 'Energy' 'Capital Goods'\n",
            " 'Basic Industries' 'Miscellaneous' 'Consumer Durables' 'Transportation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VhZ-8bEWNyiq",
        "colab_type": "code",
        "outputId": "8f8153f8-09e7-49df-9019-565978c31a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "cell_type": "code",
      "source": [
        "sectors_industries.loc[\"Consumer Non-Durables\"].sort_values(by=\"ticker\",ascending=False).head(10).index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Packaged Foods', 'Apparel', 'Beverages (Production/Distribution)',\n",
              "       'Farming/Seeds/Milling', 'Recreational Products/Toys',\n",
              "       'Plastic Products', 'Food Distributors', 'Shoe Manufacturing',\n",
              "       'Package Goods/Cosmetics', 'Meat/Poultry/Fish'],\n",
              "      dtype='object', name='Industry')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "athwtQ8KZqF3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "### Identify Set of Keywords of Interest"
      ]
    },
    {
      "metadata": {
        "id": "dfefcmZXZmK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "industries_to_analyze = sectors_industries.loc[\"Consumer Non-Durables\"].sort_values(by=\"ticker\",ascending=False).head(10).index\n",
        "\n",
        "cat_type=\"Industry\"\n",
        "interest_date_from, interest_date_to = jan_2019, today\n",
        "reference_date_from, referece_date_to = oct_2018, jan_2019\n",
        "period_stopwords = get_stopwords_for_all_calls_during_time_period(interest_date_from, interest_date_to, 0.05)\n",
        "\n",
        "for cat_filter in industries_to_analyze:\n",
        "    new_topics, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                                                            cat_type, cat_filter, filter_reference=True, shared=True, \n",
        "                                                            additional_stopwords=period_stopwords, method=\"tfidf\", number_words_to_return=25)\n",
        "\n",
        "    print(cat_filter,\"(Num Calls:\",str(interest_calls.shape[0])+\")\")\n",
        "    pprint.pprint(\"   \".join(new_topics.index))\n",
        "    print(\"#######################################################################################\")\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4S-DYaOMTI1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "### Search for Keyword Usage"
      ]
    },
    {
      "metadata": {
        "id": "rt5BygwwTtf7",
        "colab_type": "code",
        "outputId": "9a1dcf95-4d5f-4e12-a63b-39fc04688ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "word = \"jostens\"\n",
        "cat_filter = \"Plastic Products\"\n",
        "\n",
        "##############################################################################\n",
        "cat_type=\"Industry\"\n",
        "interest_date_from, interest_date_to = jan_2019, today\n",
        "reference_date_from, referece_date_to = oct_2018, jan_2019\n",
        "\n",
        "period_stopwords = get_stopwords_for_all_calls_during_time_period(interest_date_from, interest_date_to, 0.05)\n",
        "\n",
        "new_topics, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n",
        "                                                        cat_type, cat_filter, filter_reference=True, shared=True, \n",
        "                                                        additional_stopwords=period_stopwords, method=\"tfidf\", number_words_to_return=25)\n",
        "\n",
        "this_period_paragraphs = df_text_prepremarks[df_text_prepremarks.apply(lambda row: (interest_date_to > row[\"call date\"] >= interest_date_from), axis=1)]\n",
        "docs=list(this_period_paragraphs[\"Text\"])\n",
        "vectorizer = CountVectorizer(binary=True, lowercase=True, vocabulary=new_topics.index)\n",
        "vectors = vectorizer.fit_transform(docs)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "this_period_vector = pd.DataFrame(data=vectors.toarray(), index=list(this_period_paragraphs[\"Text\"].index), columns=vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "print(\"Word:\",word)\n",
        "    \n",
        "paragraphs_with_word = interest_calls[(interest_calls[\"Prepared Remarks Text\"].str.contains(word, flags=re.IGNORECASE, regex=True))]\n",
        "unique_calls_with_word_industry = paragraphs_with_word[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n",
        "print(\"    Number of Calls with Word in Group This Period:\",len(unique_calls_with_word_industry))\n",
        "print(\"        Tickers with Word in Group This Period:\",set(unique_calls_with_word_industry[\"ticker\"].drop_duplicates()))\n",
        "\n",
        "paragraphs_with_word_all_calls_this_period = this_period_paragraphs.loc[this_period_vector[this_period_vector[word]==1].index]\n",
        "unique_calls_with_word_this_period = paragraphs_with_word_all_calls_this_period[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n",
        "print(\"    Number of Calls with Word This Period (All Calls):\",len(unique_calls_with_word_this_period))\n",
        "print(\"        Tickers with Word This Period (All Calls):\",set(unique_calls_with_word_this_period[\"ticker\"].drop_duplicates()))\n",
        "\n",
        "#     paragraphs_with_word_all_calls = df_text_prepremarks[(df_text_prepremarks[\"Text\"].str.contains(word, flags=re.IGNORECASE, regex=True))]    \n",
        "#     unique_calls_with_word = paragraphs_with_word_all_calls[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n",
        "#     print(\"    Number of Calls with Word (All Calls, All Time):\",len(unique_calls_with_word))\n",
        "#     print(\"        Tickers with Word (All Calls, All Time):\",set(unique_calls_with_word[\"ticker\"].drop_duplicates()))\n",
        "   \n",
        "\n",
        "print()\n",
        "for _, row in unique_calls_with_word_this_period.iterrows():\n",
        "    print(row[\"company\"])\n",
        "    print(row[\"Link\"])\n",
        "    pprint.pprint(list(paragraphs_with_word_all_calls_this_period[paragraphs_with_word_all_calls_this_period[\"Link\"]==row[\"Link\"]][\"Text\"]))\n",
        "    print()\n",
        "\n",
        "print()\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word: jostens\n",
            "    Number of Calls with Word in Group This Period: 1\n",
            "        Tickers with Word in Group This Period: {'NWL'}\n",
            "    Number of Calls with Word This Period (All Calls): 1\n",
            "        Tickers with Word This Period (All Calls): {'NWL'}\n",
            "\n",
            "Newell Brands Inc.\n",
            "https://www.fool.com/earnings/call-transcripts/2019/02/15/newell-brands-inc-nwl-q4-2018-earnings-conference.aspx\n",
            "['On cash, we delivered about $500 million of operating cash flow, bringing '\n",
            " 'our second half operating cash flow to nearly $1.1 billion. This result was '\n",
            " 'less than what we forecast, driven in part by the completion -- the '\n",
            " 'completion timing of the Jostens and Pure Fishing yields, higher cash taxes '\n",
            " 'and transaction related costs, as well as lower accounts payable balance. We '\n",
            " 'expect to deliver more sustainable performance on payables going forward as '\n",
            " \"we're making very good progress, integrating extended payment terms into our \"\n",
            " 'sourced finished goods contracts. This work began in earnest in late 2017 '\n",
            " 'through our procurement organization, with measurable progress in 2018 and '\n",
            " 'more to come over the next two years. With respect to portfolio changes in '\n",
            " 'the quarter, we completed the divestitures of Jostens Inc., passing back '\n",
            " 'over $1 billion to shareholders through repurchases and dividends, while '\n",
            " 'reducing debt and exiting the year at our targeted leverage ratio of 3.5 '\n",
            " 'times.',\n",
            " 'Net interest expense of $104 million was down from $116 million a year ago '\n",
            " 'as we ended the year with a net debt balance of $6.5 billion as compared to '\n",
            " 'approximately $10.1 billion a year ago. In 2018, we made significant '\n",
            " 'progress strengthening the balance sheet by deleveraging. The normalized tax '\n",
            " 'rate was negative 30%, favorable relative to the year-ago rate of 3.7%, '\n",
            " 'reflecting discrete tax items. Normalized net income from discontinued '\n",
            " 'operations was $107 million, down from $196 million in the year-ago quarter, '\n",
            " 'largely due to the loss of contribution from businesses that have been '\n",
            " 'divested throughout the year, including Waddington, Rawlings, Goody, Pure '\n",
            " 'Fishing and Jostens.',\n",
            " 'In Q4, the business generated operating cash flow of $498 million compared '\n",
            " 'to $990 million a year ago, largely due to loss of cash flow from businesses '\n",
            " 'that have already been divested, higher cash taxes in transaction related '\n",
            " 'costs, as well as the lower accounts payable balance. During the fourth '\n",
            " 'quarter, we announced and closed on two transactions, Jostens and Pure '\n",
            " 'Fishing, and applied the proceeds to share buyback as well as deleveraging. '\n",
            " 'We successfully completed tender offers for over $2.6 billion of debt, and '\n",
            " \"reached the Company's targeted leverage ratio of 3.5 times in 2018. During \"\n",
            " 'Q4, we also returned $1.1 billion to shareholders through share repurchases '\n",
            " 'and dividends with the full year figure at over $1.9 billion.']\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}