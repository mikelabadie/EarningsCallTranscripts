{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9. Topic Builder.ipynb","version":"0.3.2","provenance":[{"file_id":"19mpNz7MWTdxAZ_zHq34Ohh9Kuu_cULh7","timestamp":1555075691328},{"file_id":"1mzS2wTqwPKhTLpvxrt-jExzV_76yjYh-","timestamp":1554846682393}],"collapsed_sections":["9A3Ujv2QqOCn","L35fZdIEDa39","5O0UPGv5YqXn","eRpOgCsbLJTs","5k8oRrtyCAZd","6A3ukz8aX2U9","pIbaHaWB3tO2","AhJ2lc3G3vTv","l1OzMJAKnUvs"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"EmYPO_Bwxnql","colab_type":"code","colab":{}},"cell_type":"code","source":["cat_type=\"Industry\"\n","cat_filter=\"Clothing/Shoe/Accessory Stores\"\n","interest_date_from, interest_date_to = \"2019-1-1\", \"2019-4-1\"\n","reference_date_from, referece_date_to = \"2018-10-1\", \"2019-1-1\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"9A3Ujv2QqOCn","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","## Setup\n","Includes much of the code from \"8. Keywords.ipynb\"\n","\n"]},{"metadata":{"id":"8xR7-vsGynsk","colab_type":"code","colab":{}},"cell_type":"code","source":["from datetime import datetime\n","interest_date_from, interest_date_to = datetime.strptime(interest_date_from, '%Y-%m-%d'), datetime.strptime(interest_date_to, '%Y-%m-%d')\n","reference_date_from, referece_date_to = datetime.strptime(reference_date_from, '%Y-%m-%d'), datetime.strptime(referece_date_to, '%Y-%m-%d')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L35fZdIEDa39","colab_type":"text"},"cell_type":"markdown","source":["##### Bring in Data"]},{"metadata":{"id":"-tH_oi6IrQz-","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import pprint\n","import nltk\n","nltk.download('punkt')\n","\n","import re\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import os\n","from IPython.display import clear_output\n","clear_output()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EhM5JD-kqJw7","colab_type":"code","colab":{}},"cell_type":"code","source":["data_directory = \"/content/gdrive/My Drive/DATS6450 - Labadie - Data/\"\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","if os.path.exists(data_directory+\"Pickles/metadata.pkl\"):\n","    pfile = open(data_directory+\"Pickles/metadata.pkl\", \"rb\")\n","    df_metadata = pickle.load(pfile)                 \n","    pfile.close()\n","\n","if os.path.exists(data_directory+\"Pickles/text_paragraph.pkl\"):\n","    pfile = open(data_directory+\"Pickles/text_paragraph.pkl\", \"rb\")\n","    df_text_prepremarks = pickle.load(pfile)                 \n","    pfile.close()\n","\n","clear_output()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PJz9vBpQiO2p","colab_type":"code","colab":{}},"cell_type":"code","source":["! pip install summa\n","from summa import summarizer\n","clear_output()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5O0UPGv5YqXn","colab_type":"text"},"cell_type":"markdown","source":["##### Segment Calls by Quarter"]},{"metadata":{"id":"oGfXRb_hG8Kc","colab_type":"code","outputId":"cb0cb184-9e50-44a0-edf7-0ae89691777f","executionInfo":{"status":"ok","timestamp":1555177803736,"user_tz":240,"elapsed":7727,"user":{"displayName":"Mike Labadie","photoUrl":"","userId":"14994883565526503201"}},"colab":{"base_uri":"https://localhost:8080/","height":135}},"cell_type":"code","source":["from datetime import datetime\n","from dateutil.parser import parse\n","\n","now = datetime.now()\n","today = datetime(now.year, now.month, now.day)\n","apr_2019 = datetime(2019,4,1)\n","jan_2019 = datetime(2019,1,1)\n","oct_2018 = datetime(2018,10,1)\n","jul_2018 = datetime(2018,7,1)\n","apr_2018 = datetime(2018,4,1)\n","jan_2018 = datetime(2018,1,1)\n","oct_2017 = datetime(2017,10,1)\n","\n","q4_2017_calls = df_metadata[df_metadata.apply(lambda row: (jan_2018 > row[\"call date\"] >= oct_2017), axis=1)]\n","q1_2018_calls = df_metadata[df_metadata.apply(lambda row: (apr_2018 > row[\"call date\"] >= jan_2018), axis=1)]\n","q2_2018_calls = df_metadata[df_metadata.apply(lambda row: (jul_2018 > row[\"call date\"] >= apr_2018), axis=1)]\n","q3_2018_calls = df_metadata[df_metadata.apply(lambda row: (oct_2018 > row[\"call date\"] >= jul_2018), axis=1)]\n","q4_2018_calls = df_metadata[df_metadata.apply(lambda row: (jan_2019 > row[\"call date\"] >= oct_2018), axis=1)]\n","q1_2019_calls = df_metadata[df_metadata.apply(lambda row: (row[\"call date\"] >= jan_2019), axis=1)]\n","\n","print(\"Q4 2017 Calls:\",q4_2017_calls.shape)\n","print(\"Q1 2018 Calls:\",q1_2018_calls.shape)\n","print(\"Q2 2018 Calls:\",q2_2018_calls.shape)\n","print(\"Q3 2018 Calls:\",q3_2018_calls.shape)\n","print(\"Q4 2018 Calls:\",q4_2018_calls.shape)\n","print(\"Q1 2019 Calls:\",q1_2019_calls.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Q4 2017 Calls: (137, 16)\n","Q1 2018 Calls: (306, 16)\n","Q2 2018 Calls: (355, 16)\n","Q3 2018 Calls: (546, 16)\n","Q4 2018 Calls: (1404, 16)\n","Q1 2019 Calls: (2328, 16)\n"],"name":"stdout"}]},{"metadata":{"id":"L25ge-GrCzSb","colab_type":"code","outputId":"9510f22d-fd19-45df-9067-b99746cb6803","executionInfo":{"status":"ok","timestamp":1555177814387,"user_tz":240,"elapsed":18377,"user":{"displayName":"Mike Labadie","photoUrl":"","userId":"14994883565526503201"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["docs = df_metadata[\"Prepared Remarks Text\"]\n","def get_vocab(docs):\n","    vectorizer = CountVectorizer(binary=True, lowercase=True)\n","    vectors = vectorizer.fit_transform(docs)\n","    return vectorizer.get_feature_names()  \n","  \n","full_vocab = get_vocab(docs)\n","len(full_vocab)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65354"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"eRpOgCsbLJTs","colab_type":"text"},"cell_type":"markdown","source":["##### Get Industries + Sectors"]},{"metadata":{"id":"xFZc4POCMOV5","colab_type":"code","colab":{}},"cell_type":"code","source":["sectors=df_metadata[\"Sector\"].dropna().drop_duplicates().values\n","industries=df_metadata[\"Industry\"].dropna().drop_duplicates().values\n","sectors_industries = df_metadata.dropna(subset=[\"Sector\",\"Industry\"]).groupby([\"Sector\",\"Industry\"]).agg({\"ticker\":pd.Series.nunique})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5k8oRrtyCAZd","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","### Build Generic Earnings Call Stop Word Sets at Call/Paragraph Levels\n","* Words that are in 75% of calls\n","* Words that are in 7.5% of paragraphs"]},{"metadata":{"id":"GUUfvoZ23bpX","colab_type":"code","outputId":"514fdac6-2172-4789-d189-30b88b9abe9b","executionInfo":{"status":"ok","timestamp":1555177837240,"user_tz":240,"elapsed":41229,"user":{"displayName":"Mike Labadie","photoUrl":"","userId":"14994883565526503201"}},"colab":{"base_uri":"https://localhost:8080/","height":590}},"cell_type":"code","source":["# build high-level stop word list at the entire call level\n","docs = df_metadata[\"Prepared Remarks Text\"]\n","vectorizer = CountVectorizer(binary=True, lowercase=True, min_df=0.75)\n","vectors = vectorizer.fit_transform(docs)\n","feature_names = vectorizer.get_feature_names()\n","results = pd.DataFrame(data=vectors.toarray(), index=docs.index, columns=vectorizer.get_feature_names())\n","call_stopwords = results.sum(axis=0)\n","call_stopwords = list(call_stopwords.index)\n","print(len(call_stopwords))\n","print(np.array(call_stopwords))\n","print()\n","\n","# build high-level stop word list at the paragraph level\n","docs = df_text_prepremarks[\"Text\"]\n","vectorizer = CountVectorizer(binary=True, lowercase=True, min_df=0.075)\n","vectors = vectorizer.fit_transform(docs)\n","feature_names = vectorizer.get_feature_names()\n","results = pd.DataFrame(data=vectors.toarray(), index=docs.index, columns=vectorizer.get_feature_names())\n","paragraph_stopwords = results.sum(axis=0)\n","paragraph_stopwords = list(paragraph_stopwords.index)\n","print(len(paragraph_stopwords))\n","print(np.array(paragraph_stopwords))\n","\n","# combine stopwords from both levels\n","combined_stopwords = set(paragraph_stopwords).union(set(call_stopwords))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["161\n","['10' '12' '20' '2017' '2018' '2019' 'about' 'across' 'actual' 'addition'\n"," 'additional' 'all' 'also' 'an' 'and' 'any' 'approximately' 'are' 'as'\n"," 'at' 'back' 'based' 'basis' 'be' 'been' 'before' 'believe' 'both'\n"," 'business' 'but' 'by' 'call' 'can' 'capital' 'cash' 'company' 'compared'\n"," 'continue' 'continued' 'cost' 'current' 'driven' 'due' 'during'\n"," 'earnings' 'end' 'everyone' 'expect' 'expected' 'financial' 'first' 'for'\n"," 'forward' 'fourth' 'from' 'full' 'future' 'gaap' 'good' 'growth' 'had'\n"," 'has' 'have' 'high' 'higher' 'impact' 'in' 'including' 'income'\n"," 'increase' 'increased' 'into' 'is' 'it' 'joining' 'last' 'like' 'line'\n"," 'll' 'long' 'looking' 'lower' 'made' 'margin' 'market' 'may' 'me'\n"," 'million' 'more' 'most' 'net' 'new' 'non' 'not' 'now' 'of' 'on' 'one'\n"," 'operating' 'or' 'other' 'our' 'over' 'per' 'performance' 'primarily'\n"," 'prior' 'provide' 'quarter' 'questions' 'range' 'rate' 're' 'related'\n"," 'release' 'result' 'results' 'revenue' 'sales' 'second' 'see' 'share'\n"," 'significant' 'some' 'statements' 'strong' 'term' 'than' 'thank' 'that'\n"," 'the' 'their' 'these' 'third' 'this' 'those' 'through' 'time' 'to'\n"," 'today' 'total' 'turn' 'two' 'up' 'us' 'value' 've' 'very' 'was' 'we'\n"," 'website' 'well' 'were' 'which' 'while' 'will' 'with' 'would' 'year'\n"," 'years' 'you']\n","\n","109\n","['10' '2017' '2018' '2019' 'about' 'adjusted' 'all' 'also' 'an' 'and'\n"," 'approximately' 'are' 'as' 'at' 'basis' 'be' 'both' 'business' 'but' 'by'\n"," 'call' 'cash' 'company' 'compared' 'continue' 'customers' 'due' 'during'\n"," 'earnings' 'end' 'expect' 'financial' 'first' 'for' 'forward' 'fourth'\n"," 'from' 'full' 'good' 'growth' 'has' 'have' 'higher' 'impact' 'in'\n"," 'including' 'increase' 'increased' 'into' 'is' 'it' 'last' 'like' 'll'\n"," 'margin' 'market' 'million' 'more' 'net' 'new' 'not' 'now' 'of' 'on'\n"," 'one' 'operating' 'or' 'other' 'our' 'over' 'per' 'performance' 'quarter'\n"," 'rate' 're' 'results' 'revenue' 'sales' 'second' 'share' 'so' 'some'\n"," 'strong' 'than' 'that' 'the' 'these' 'third' 'this' 'through' 'time' 'to'\n"," 'today' 'total' 'turn' 'up' 'us' 've' 'very' 'was' 'we' 'well' 'were'\n"," 'which' 'while' 'will' 'with' 'year' 'you']\n"],"name":"stdout"}]},{"metadata":{"id":"6A3ukz8aX2U9","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","### Word Frequency Functions\n","*   This is where the magic happens.\n","\n"]},{"metadata":{"id":"pIbaHaWB3tO2","colab_type":"text"},"cell_type":"markdown","source":["#### Helpers\n","Many of these are not currently used, but were at one point useful during development."]},{"metadata":{"id":"PEnfD4CSTAEc","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_calls(from_date, to_date, category=None, category_filter=None):\n","    calls = df_metadata[df_metadata.apply(lambda row: (to_date > row[\"call date\"] >= from_date), axis=1)]\n","    if category is not None:\n","        calls = calls[calls[category]==category_filter]\n","    return calls\n","\n","def get_calls_for_shared_tickers(calls1, calls2):\n","    tickers = list(set(calls2[\"ticker\"]).intersection(set(calls1[\"ticker\"])))\n","    calls1 = calls1[calls1[\"ticker\"].isin(tickers)]\n","    calls2 = calls2[calls2[\"ticker\"].isin(tickers)]\n","    return calls1, calls2\n","\n","def get_calls_interest_reference(interest_date_from, interest_date_to, reference_date_from, referece_date_to, cat_type, cat_filter, shared=False):\n","    interest_calls = get_calls(interest_date_from, interest_date_to, cat_type, cat_filter)\n","    reference_calls = get_calls(reference_date_from, referece_date_to, cat_type, cat_filter)\n","    if shared:\n","        reference_calls, interest_calls = get_calls_for_shared_tickers(reference_calls, interest_calls)\n","    return interest_calls, reference_calls \n","  \n","def get_norm_count_vector(docs, index, vocab, stopwords):\n","    vectorizer = CountVectorizer(binary=False, lowercase=True, vocabulary=vocab, stop_words=stopwords)\n","    vectors = vectorizer.fit_transform(docs)\n","    feature_names = vectorizer.get_feature_names()\n","    results = pd.DataFrame(data=vectors.toarray(), index=index, columns=vectorizer.get_feature_names())\n","    results=results.div(results.sum(axis=1), axis=0)\n","    results.fillna(0,inplace=True)\n","    return results\n","  \n","def get_tfidf_vector(docs, index, vocab, stopwords):\n","    vectorizer = TfidfVectorizer(binary=False, lowercase=True, vocabulary=vocab, stop_words=stopwords)\n","    vectors = vectorizer.fit_transform(docs)\n","    feature_names = vectorizer.get_feature_names()\n","    results = pd.DataFrame(data=vectors.toarray(), index=index, columns=vectorizer.get_feature_names())\n","    results.fillna(0,inplace=True)\n","    return results\n","  \n","# get top 20 changes up and down\n","def get_keywords_norm_count(first_qtr_calls, second_qtr_calls):\n","    # build relative frequency word vector for first period\n","    docs = first_qtr_calls[\"Prepared Remarks Text\"]\n","    first_qtr_results = get_norm_count_vector(docs,first_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n","\n","    # build relative frequency word vector for second period\n","    docs = second_qtr_calls[\"Prepared Remarks Text\"]\n","    second_qtr_results = get_norm_count_vector(docs,second_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n","\n","    # get the difference in relative frequency by company/word\n","    diff = (second_qtr_results.mean() - first_qtr_results.mean()).sort_values()\n","    return diff.head(20).append(diff.tail(20))\n","  \n","# get top 20 changes up and down\n","def get_keywords_tfidf(first_qtr_calls, second_qtr_calls):\n","    # build relative frequency word vector for first quarter\n","    docs = first_qtr_calls[\"Prepared Remarks Text\"]\n","    first_qtr_results = get_tfidf_vector(docs, first_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n","\n","    # build relative frequency word vector for second quarter\n","    docs = second_qtr_calls[\"Prepared Remarks Text\"]\n","    second_qtr_results = get_tfidf_vector(docs, second_qtr_calls[\"ticker\"], full_vocab, combined_stopwords)\n","\n","    # get the difference in relative frequency by company/word\n","    diff = (second_qtr_results.mean() - first_qtr_results.mean()).sort_values()\n","    return diff.head(20), diff.tail(20).sort_values(ascending=False)\n","\n","\n","def get_count_vector(docs, index, vocab, stopwords):\n","    vectorizer = CountVectorizer(binary=False, lowercase=True, vocabulary=vocab, stop_words=stopwords)\n","    vectors = vectorizer.fit_transform(docs)\n","    feature_names = vectorizer.get_feature_names()\n","    results = pd.DataFrame(data=vectors.toarray(), index=index, columns=vectorizer.get_feature_names())\n","    results.fillna(0,inplace=True)\n","    return results\n","\n","# get top 20 changes up and down\n","def get_keywords_count(first_qtr_calls, second_qtr_calls, additional_stopwords=[]): \n","    # build relative frequency word vector for first period\n","    docs = first_qtr_calls[\"Prepared Remarks Text\"]\n","    first_qtr_results = get_count_vector(docs,first_qtr_calls[\"ticker\"], \n","                                         full_vocab, list(combined_stopwords)+additional_stopwords)\n","\n","    # build relative frequency word vector for second period\n","    docs = second_qtr_calls[\"Prepared Remarks Text\"]\n","    second_qtr_results = get_count_vector(docs,second_qtr_calls[\"ticker\"], \n","                                          full_vocab, list(combined_stopwords)+additional_stopwords)\n","\n","    # get the difference in relative frequency by company/word\n","    diff = (second_qtr_results.sum() - first_qtr_results.sum()).sort_values()\n","    return diff.head(20), diff.tail(20).sort_values(ascending=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AhJ2lc3G3vTv","colab_type":"text"},"cell_type":"markdown","source":["#### Primary Functions"]},{"metadata":{"id":"g6SAldyHrUyD","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_stopwords_for_all_calls_during_time_period(interest_date_from, interest_date_to,relative_freq_to_max=0.5):\n","    calls = get_calls(interest_date_from, interest_date_to)\n","    docs=calls[\"Prepared Remarks Text\"]\n","    all_results = get_count_vector(docs,calls[\"ticker\"], full_vocab, combined_stopwords)\n","    all_results = all_results.sum().sort_values(ascending=False)\n","    num=all_results[0]*relative_freq_to_max\n","    all_results = list(all_results[all_results>num].index)\n","    return all_results\n","    \n","def get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n","                               cat_type, cat_filter, \n","                               filter_reference=True, shared=False, additional_stopwords=None, \n","                               method=\"counts\", number_words_to_return=None):\n","  \n","    # build the sets of calls to compare\n","    interest_calls = get_calls(interest_date_from, interest_date_to, cat_type, cat_filter)\n","    \n","    \n","    # should the reference period use the same filter\n","    if filter_reference:\n","      reference_calls = get_calls(reference_date_from, referece_date_to, cat_type, cat_filter)\n","    else:\n","      reference_calls = get_calls(reference_date_from, referece_date_to, None, None)\n","    \n","    # remove interest calls from the reference calls\n","    reference_calls = reference_calls[~(reference_calls[\"Link\"].isin(interest_calls[\"Link\"]))]\n","    \n","    # filter to calls with tickers in both reference and interest\n","    if shared:\n","        reference_calls, interest_calls = get_calls_for_shared_tickers(reference_calls, interest_calls)\n","    \n","    \n","    # determine what method will be used to identify changes\n","    if method==\"counts\":\n","        docs=reference_calls[\"Prepared Remarks Text\"]\n","        reference_results = get_count_vector(docs,reference_calls[\"ticker\"], full_vocab, combined_stopwords)\n","        reference_results = reference_results.sum().sort_values(ascending=False)\n","\n","        docs=interest_calls[\"Prepared Remarks Text\"]\n","        interest_results = get_count_vector(docs, interest_calls[\"ticker\"], full_vocab, combined_stopwords)\n","        interest_results = interest_results.sum().sort_values(ascending=False)\n","        \n","        # convert counts to percent of total and get difference\n","        interest_results_pct=interest_results.div(interest_results.sum())\n","        reference_results_pct=reference_results.div(reference_results.sum())\n","        diff = interest_results_pct - reference_results_pct        \n","    elif method == \"tfidf\":\n","        reference_doc=\" \".join(reference_calls[\"Prepared Remarks Text\"])\n","        interest_doc=\" \".join(interest_calls[\"Prepared Remarks Text\"])\n","        docs = [reference_doc,interest_doc]\n","\n","        vectorizer = TfidfVectorizer(binary=False, lowercase=True, stop_words=combined_stopwords)\n","        vectors = vectorizer.fit_transform(docs)\n","        feature_names = vectorizer.get_feature_names()\n","        results = pd.DataFrame(data=vectors.toarray(), columns=vectorizer.get_feature_names())\n","        results.fillna(0,inplace=True)\n","        diff = results.loc[1]-results.loc[0]\n","    \n","    \n","    # drop words from results\n","    if additional_stopwords is not None:\n","        diff=diff.drop(additional_stopwords, errors=\"ignore\")\n","    \n","    diff = diff.sort_values(ascending=False)\n","    if number_words_to_return is not None:  \n","        diff = diff.head(number_words_to_return)\n","    \n","    return diff, interest_calls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BaMOq3JxqObs","colab_type":"text"},"cell_type":"markdown","source":["### Build Paragraph List (and Word Vector) for Period of Interest"]},{"metadata":{"id":"4cGQLIlkZfx2","colab_type":"code","colab":{}},"cell_type":"code","source":["period_stopwords = get_stopwords_for_all_calls_during_time_period(interest_date_from, interest_date_to, 0.05)\n","\n","new_topics, interest_calls = get_keywords_by_comparison(interest_date_from, interest_date_to, reference_date_from, referece_date_to, \n","                                                        cat_type, cat_filter, filter_reference=True, shared=True, \n","                                                        additional_stopwords=period_stopwords, method=\"tfidf\", number_words_to_return=25)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gPP-zlotqMvz","colab_type":"code","colab":{}},"cell_type":"code","source":["this_period_paragraphs = df_text_prepremarks[df_text_prepremarks.apply(lambda row: (interest_date_to > row[\"call date\"] >= interest_date_from), axis=1)]\n","docs=list(this_period_paragraphs[\"Text\"])\n","vectorizer = CountVectorizer(binary=True, lowercase=True, vocabulary=new_topics.index)\n","vectors = vectorizer.fit_transform(docs)\n","feature_names = vectorizer.get_feature_names()\n","this_period_vector = pd.DataFrame(data=vectors.toarray(), index=list(this_period_paragraphs[\"Text\"].index), columns=vectorizer.get_feature_names())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7eahHNDylkxk","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","## Summarization and TextRank\n","Now that you have some keywords, can you identify/compile text to give understanding of why those words are important?\n","* text-rank summarization using summanlp\n","  * no parameters:  just gave me the first sentence\n","  * 50 words: interesting!"]},{"metadata":{"id":"3Sc-sOeXipLj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2958},"outputId":"f89e063b-8f6e-43cf-9520-0c97987667f2","executionInfo":{"status":"ok","timestamp":1555178431234,"user_tz":240,"elapsed":827,"user":{"displayName":"Mike Labadie","photoUrl":"","userId":"14994883565526503201"}}},"cell_type":"code","source":["for word in new_topics.index[0:1]:   \n","    print(\"Word:\",word)\n","    \n","    paragraphs_with_word = interest_calls[(interest_calls[\"Prepared Remarks Text\"].str.contains(word, flags=re.IGNORECASE, regex=True))]\n","    unique_calls_with_word_industry = paragraphs_with_word[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n","    print(\"    Number of Calls with Word in Group This Period:\",len(unique_calls_with_word_industry))\n","    print(\"        Tickers with Word in Group This Period:\",set(unique_calls_with_word_industry[\"ticker\"].drop_duplicates()))\n","    \n","    paragraphs_with_word_all_calls_this_period = this_period_paragraphs.loc[this_period_vector[this_period_vector[word]==1].index]\n","    unique_calls_with_word_this_period = paragraphs_with_word_all_calls_this_period[[\"company\",\"ticker\",\"Link\"]].drop_duplicates()\n","    print(\"    Number of Calls with Word This Period (All Calls):\",len(unique_calls_with_word_this_period))\n","    print(\"        Tickers with Word This Period (All Calls):\",set(unique_calls_with_word_this_period[\"ticker\"].drop_duplicates()))\n","\n","    print()\n","    for _, row in unique_calls_with_word_this_period.iterrows():\n","        print(row[\"company\"])\n","        print(row[\"Link\"])\n","        text = list(paragraphs_with_word_all_calls_this_period[paragraphs_with_word_all_calls_this_period[\"Link\"]==row[\"Link\"]][\"Text\"])\n","        t = \" \".join(text)\n","        \n","        if len(text) < 5:\n","            print(\"Summary at Call Level\")\n","            pprint.pprint(summarizer.summarize(t, words=100))\n","        else:\n","            print(\"Summary of Each Paragraph\")\n","            for t in text:\n","                summary = summarizer.summarize(t, words=25)\n","                if summary: pprint.pprint(summary); print()\n","        print()\n","    print()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Word: gymboree\n","    Number of Calls with Word in Group This Period: 1\n","        Tickers with Word in Group This Period: {'PLCE'}\n","    Number of Calls with Word This Period (All Calls): 7\n","        Tickers with Word This Period (All Calls): {'RPAI', 'PLCE', 'TCO', 'RPT', 'VRS', 'SKT', 'RILY'}\n","\n","Retail Properties of America Inc\n","https://www.fool.com/earnings/call-transcripts/2019/02/13/retail-properties-of-america-inc-rpai-q4-2018-earn.aspx\n","Summary at Call Level\n","('As it relates to our expectations for same-store and leasing trends for '\n"," '2019, we have seen several smaller bankruptcies this year, including Shopko '\n"," 'Gymboree, things remembered beauty brands and Charlotte Russe.\\n'\n"," 'And while we have been relatively unaffected to-date, we remain cautious on '\n"," 'our watch list and same-store NOI growth assumption.\\n'\n"," 'In addition to our 50 basis points of bad debt embedded in that assumption, '\n"," 'we have assumed that a small number of our watch list tenants do not renew '\n"," 'certain locations this year based on our credit surveillance and early '\n"," 'discussion around renewals and/or remerchandising and mark-to-market '\n"," 'opportunities on those spaces.')\n","\n","Taubman Centers Inc\n","https://www.fool.com/earnings/call-transcripts/2019/02/14/taubman-centers-inc-tco-q4-2018-earnings-conferenc.aspx\n","Summary at Call Level\n","('Over the last 12 months, following the elevated number of store closures '\n"," 'experienced throughout late 2016 and 2017, the retail landscape seems to be '\n"," 'stabilizing for higher-quality assets.\\n'\n"," 'Gymboree has three brands.\\n'\n"," 'We have nine Janie and Jacks, eight US Gymborees, and one Crazy 8, we have '\n"," 'six Charlotte Russes, and seven Things Remembered.\\n'\n"," \"It's true that bankruptcies create vacancies, but with high-quality real \"\n"," 'estate, they also provide the opportunity to improve the merchandising of '\n"," 'the center.\\n'\n"," 'At our fourth quarter call last year, we mentioned 40 deals with emerging '\n"," 'brands that were added to our portfolio throughout 2017.')\n","\n","Tanger Factory Outlet Centers\n","https://www.fool.com/earnings/call-transcripts/2019/02/15/tanger-factory-outlet-centers-skt-q4-2018-earnings.aspx\n","Summary at Call Level\n","('Two retailers in our portfolio have recently declared bankruptcy.\\n'\n"," 'In our consolidated portfolio, we have a total of 28 Gymboree stores, '\n"," 'including their Crazy 8 concept, comprising 66,000 square feet, and 15 '\n"," 'Charlotte Russe locations, comprising 87,000 square feet.\\n'\n"," 'I want to emphasize that these are still fluid situations, and we do not '\n"," 'know exactly which stores will close and when.\\n'\n"," 'As we seek to fill vacancies, we have the opportunity to upgrade our tenant '\n"," 'mix with vibrant new retailers.')\n","\n","RPT Realty\n","https://www.fool.com/earnings/call-transcripts/2019/02/21/rpt-realty-rpt-q4-2018-earnings-conference-call-tr.aspx\n","Summary at Call Level\n","('Lastly, while unanticipated store closures could impact our outlook, we do '\n"," 'take a surgical approach to our budgeting process and have assumed that some '\n"," 'of our at-risk tenants will not renew in 2019.\\n'\n"," 'In addition, our model assumes bad debt of approximately 60 basis points of '\n"," 'same-property NOI or $892,000 to capture any fallout from unanticipated '\n"," 'tenant closures.\\n'\n"," 'While store closures remain an uncertainty, we do not have any Sears or '\n"," 'Kmarts in our portfolio and the bankruptcies of Gymboree, Mattress Firm and '\n"," '(inaudible) have very little impact on our business.\\n'\n"," 'Regarding Payless, we only have four locations.')\n","\n","Verso Corp\n","https://www.fool.com/earnings/call-transcripts/2019/02/28/verso-corp-vrs-q4-2018-earnings-conference-call-tr.aspx\n","Summary at Call Level\n","(\"Volume and order entry was very strong in '18, recently we're starting to \"\n"," 'see some slowdown in the order entry activity.\\n'\n"," 'We would attribute that to general concerns about the economy globally.\\n'\n"," 'And the inventory levels are higher now than they were as we exited 2018.\\n'\n"," \"And we're seeing more and more signs of a tough retail environment going \"\n"," 'forward.\\n'\n"," 'So, a lot of closures have been announced from Payless to Gymboree, ShopKo '\n"," 'and other stores.')\n","\n","B. Riley Financial Inc\n","https://www.fool.com/earnings/call-transcripts/2019/03/05/b-riley-financial-inc-rily-q4-2018-earnings-confer.aspx\n","Summary at Call Level\n","('Momentum in this business is carrying forward into 2019 as a liquidation of '\n"," 'Bon-Ton real estate assets continues to be under way and with our recently '\n"," 'announced participation in the liquidations of Gymboree and Payless Shoes.\\n'\n"," 'The Payless store closing event, which began on February 17, is the largest '\n"," 'liquidation by store count in retail history with sales being conducted at '\n"," 'approximately 2,100 stores and associated inventory value at over $1 '\n"," 'billion.\\n'\n"," 'We expect to realize significant contributions from the Bon-Ton liquidation '\n"," 'results for the first half, in addition to the results from our current '\n"," 'involvement in Gymboree and Payless liquidations.')\n","\n","Children's Place Inc\n","https://www.fool.com/earnings/call-transcripts/2019/03/05/childrens-place-inc-plce-q4-2018-earnings-conferen.aspx\n","Summary of Each Paragraph\n","(\"Now let's discuss the recently announced agreement to purchase the IP Assets \"\n"," 'of Gymboree and Crazy 8.')\n","\n","('We believe that the acquisition of the Gymboree brand and its legacy '\n"," 'strength in the toddler sizes will provide us with the unique opportunity to '\n"," 'expand our market share in this important size range.')\n","\n","(\"The Gymboree mom like the Children's Place mom is digitally savvy and chose \"\n"," 'the preference for omnichannel shopping experiences and we are now better '\n"," 'prepared to bring these customers into our digital ecosystem.')\n","\n","('We have already identified sites to be addressed and have started to execute '\n"," 'on our real estate plan which will add incremental sales and profits to our '\n"," 'accretion forecast over time.')\n","\n","('Moving on to Q4.\\n'\n"," 'In 2018, we recognized that our closest specialty competitor was distressed '\n"," 'and with our stores overlapping nearly 70% of their approximately 800 '\n"," 'remaining locations, we were presented with unique market share opportunity.')\n","\n","('Despite the solid start, we felt it was prudent at that time to discuss the '\n"," 'high likelihood of further distress for TCP and the potential for '\n"," 'unprecedented near-term competitive pressure as presenting a meaningful risk '\n"," 'to profitability.')\n","\n","('Based on this new information, we took strategic action in the fourth '\n"," 'quarter to significantly accelerate the liquidation of our seasonal '\n"," \"carryover inventory in response to Gymboree's plans to liquidate all of its \"\n"," 'stores in a compressed Q1 timeframe.')\n","\n","('Although, this accelerated liquidation adversely impacted our fourth quarter '\n"," 'EPS, it allowed us to minimize the adverse margin impact, that we would have '\n"," 'otherwise experienced, had we attempted and potentially failed to liquidate '\n"," 'our seasonal carryover product in Q1, during the Gymboree liquidation.')\n","\n","('And as a result, we have much greater exposure to the disruptive impact of '\n"," 'the Gymboree closures, but we are also much better positioned to capture the '\n"," 'market share benefit beyond the closings.')\n","\n","(\"We currently estimate an adverse impact from Gymboree's liquidation events \"\n"," 'in the first half.\\n'\n"," 'We anticipate seeing the benefits from the Gymboree liquidation beginning in '\n"," 'the back half of 2019.')\n","\n","('We anticipate this group of capital constrained, poorly positioned and/or '\n"," 'over-stored retailers will continue to be forced to consolidate and shutter '\n"," 'doors, and if we continue to successfully execute on our long-term strategy, '\n"," \"the current market presents the Children's Place with significant and \"\n"," 'ongoing market share opportunities.')\n","\n","('Following a discussion on the agreement to acquire the Gymboree Assets, I '\n"," 'will provide an update on our fourth quarter financials and our outlook for '\n"," '2019.')\n","\n","(\"I'd like to provide some additional context regarding the financial \"\n"," 'implications of the deal.')\n","\n","('As Jane previously mentioned, the acquisition process provided us with a '\n"," \"detailed analysis of Gymboree's real estate portfolio.\")\n","\n","('We remain a net closer of stores.\\n'\n"," 'The acquisition will provide us opportunities across various channels, '\n"," 'including e-commerce, TCP stores, wholesale and international.')\n","\n","('And four, an adverse impact of approximately $37 million in sales, resulting '\n"," 'from the calendar shift related to the 53rd week in the fourth quarter of '\n"," '2017, resulted in an adverse impact of $0.12 per diluted share.')\n","\n","('Our inventory clearance strategy resulted in approximately 290 basis points '\n"," 'of margin pressure in the fourth quarter, but will help us limit our '\n"," 'inventory exposure and minimize the adverse margin impact that we would have '\n"," 'otherwise experienced had we attempted to liquidate the product in Q1.')\n","\n","('Our inventory was down 6.5% at the end of the year which was significantly '\n"," 'lower than our flat to up low-single digit guidance as a result of our '\n"," 'strategic decision to accelerate the liquidation of our seasonal carryover '\n"," 'inventory ahead of the Gymboree Q1 liquidation event.')\n","\n","'Now let me take you through our updated outlook for 2019.\\nOutlook for 2019.'\n","\n","('We expect to generate strong cash flow from operations in 2019, which will '\n"," 'fund our capital return program and capital expenditures.')\n","\n","('The Gymboree liquidation will have a $1.15 to $1.40 adverse impact on '\n"," 'adjusted EPS.')\n","\n","('And in addition, we expect to drive margin benefits through market share '\n"," 'gains, our fleet optimization strategy, and ongoing expense management.')\n","\n","\n","\n"],"name":"stdout"}]},{"metadata":{"id":"l1OzMJAKnUvs","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","## Cross-document Structure Theory\n","Now that you have some keywords, can you identify/compile text to give understanding of why those words are important?"]},{"metadata":{"id":"fnTRtYoKrRPf","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}